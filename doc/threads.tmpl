            +-------------------+
            |       OS 211      |
            |  TASK 1: THREADS  |
            |  DESIGN DOCUMENT  |
            +-------------------+
                   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Irina-Elena Veliche <irina-elena.veliche11@imperial.ac.uk>
Paul Rowe-White <paul.rowe-white11@imperial.ac.uk>
Andrei Bogdan Antonescu <andrei.antonescu11@imperial.ac.uk>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

New Struct:

The sleeping thread struct is used to record sleeping threads so that they 
can be blocked until the time they want to be woken. 
/* Struct for representing a thread that is sleeping */
struct sleeping_thread
  {
    int64_t wake_at_ticks;      /* Number of ticks since CPU 
                                   start to be woken up at */
    struct semaphore sema;      /* Semaphore for blocking and 
                                   unblocking the thread */ 
    struct list_elem elem;      /* List element used for traversing the list */
  };

Static Variables in timer.c:

This is the list of sleeping threads, ordered by the time to be woken at in 
ascending order.
/* List of sleeping threads */
struct list sleeping_threads_list;

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

When a thread calls timer_sleep() it is blocked until to required amount of 
ticks have passed. The time to wake up at is calculated by adding the current 
number of timer ticks to the required amount of ticks to sleep for. This is
stored in a struct sleeping_thread along with a new semaphore that is
intialised with a value of  0. The struct is then added to the list of 
sleeping threads. sema_down is then called on the new semaphore which causes 
the current thread to be blocked.

In the timer interrupt handler the list of sleeping threads is checked to see
if any threads need to be woken at the current number of ticks. If so then 
sema_up is called on the thread so that it gets unblocked. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

To minimise the time spent in the interrupt handler, the list of sleeping
threads is kept in ascending order of the time to wake. This means that when 
the list is inspected for threads to wake up, the first element in the list can
be removed and checked until a thread with a wake up time greater than the 
current number of ticks is found. When this happens there will be no more 
threads that need waking up in the list and so the interrupti handler can move 
on.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

If multiple threads call timer_sleep() simultaneously then race conditions 
are avoided by disabling interrupts around the call to list_insert_ordered() 
to insert the current thread into the list of sleeping threads.
This is required as the list is also accessed from within an interrupt handler
(see next question). As disabling interrupts is required there is no need for 
any other synchronisation mechanisms for when multiple threads call 
timer_sleep(). This is because disabling interrupts around 
list_insert_ordered() makes that operation atomic so no other threads will be
scheduled during this call. This is the only call that accesses shared data so
there is no race condition on any other operation in timer_sleeep(). 

A lock could have been used to prevent race conditions between threads 
accessing the list of sleeping threads at the same time if it was not accessed
from within an interrupt handler. 

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

If a timer interrupt occurs during a call to timer_sleep(), race conditions 
are avoided by disabling interrupts around the call to list_insert_ordered() to
insert the current thread into the list of sleeping threads.
This is because the sleeping threads list is accessed from both the 
timer_sleep() function and the timer interrupt. timer_sleep() inserts elements
in the correct position to maintain the ordering of list and the timer 
interrupt removes elements from the list. If a timer interrupt were allowed to
occur while an element was being added then one of the elements either side of
the position to insert may be deleted before the insert can be completed, 
leading to some elements having incorrect pointers.

Disabling interrupts is the only way to ensure mutual exclusion as the 
interrupt handler cannot acquire a lock or down a semaphore as it must not be 
allowed to sleep, which can happen with both of these synchronisation 
mechanisms. The time interrupts are disabled is kept to a minimum by 
re-enabling them once the call to list_insert_ordered() is finished. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

I chose this design because I feel that it is very simple and efficient. 
Maintaining an ordered list of sleeping threads means that checking each 
sleeping thread to see if it need waking up is very quick and the space 
requirements are low as only the wake up time, the thread semaphore and a 
list element need to be stored for each thread that is sleeping. The wake up 
time and the semaphore could have been put in the struct thread so every 
thread would have them and the semaphore would not have to be re-initialised
every time the same thread sleeps, it could just be reused. However, to check
what threads need waking up every thread that had been created would have to 
be checked, even those that were not asleep. This would be very inefficient 
and would always take the same amount of time, whereas the check on the 
separate list can be terminated very quickly. It is also not a very clean 
design as variables that are only used in timer_sleep() are put in the general
thread structure and increases its size on the thread memory page, which is 
small, leaving less room for the thread's stack.

A semaphore is used to do the thread blocking and unblocking instead of calling
thread_block() and thread_unblock() directly as these are lower level and
used by the semaphore anyway. 
 

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In struct thread from thread.h we added some other fields to a thread in 
order to be able to make the priority donation work.

/* Thread */
struct thread 
  {
    ...

	int base_priority;      /* Base priority of a thread. */
	bool donated;           /* If a thread has donated priority. */
	struct list locks;      /* List of locks hold by a thread */ 
	struct list *blocked;   /* The lock blocking the thread */

    ...
  }

Thus, we have a base_priority field, where we keep the priority of each thread
before it receives any donations. If no donations are received, it should have
the same priority.
We also keep a bool donated, that is false initially and becomes true if the 
thread receives a priority donation.
The struct list locks keeps the list of locks held by a thread, and in the 
blocked field we keep the lock blocking the thread if any. 

In synch.h we added some new fields to the struct of lock, to facilitate the 
priority donation.

/* Lock. */
struct lock
  {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct list_elem lock_elem; /* List elem of lock from structure thread */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    int lock_priority;          /* The highest priority waiting for the lock. */
  };


In lock_elem we keep the list element of lock from the structure thread, while
we chose to have a field lock_priority to keep the highest priority waiting
for the lock.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

In order to track priority donation we used a list from which we extract the 
element with the highest priority from the list of waiters in a semaphore.
The list_remove_ordered function removes the lowest element from the list 
according to the priority based comparation given by the list_less_func
funcion declared in synch.c.
We also had to handle nested donation. Consider we have three threads,
H, M and L, with priorities high, medium and low. If H is waiting on a lock
that M holds and M is waiting on a lock that L holds, then both M and L should
be boosted to H's priority. 

Here is a .png drawing of this case: https://www.doc.ic.ac.uk/~aba111/donate.png

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

We wrote a function in list.c that removes from a given list the max
element according to a provided less than function. Our function compares
the threads based on their priority so we will always add to the ready 
list the one with the biggest priority. Alternatively we considered 
replacing all the push_back calls with insert_ordered so maintaining
the queues ordered based on priority, but this would require to sort 
the queue each time we set a thread priority. Our solution has the 
advantage of a liner complexity with the number of threads as worst case.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When we try to aquire a lock we check if that lock is held by another thread
and if this is the case we donate the current thread's priority to the thread
that holds the lock. Nested donation could occur if the lock holder is also 
blocked by another thread holding a lock he is trying to aquire. In this case 
we will donate priority to all the threads in such a chain. We didn't impose 
any limit to priority donation because the threads list is usually quite small
and in practice nested donations are even smaller. 

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When a thread releases a lock we will set its priority either to the default
priority (before acquiring the lock) or to the max priority of the other threads
waiting for locks held by it. We do this by taking the max lock when compared
to priority from a thread's locks list. After we set a priority we check to see 
if there is another thread in the ready list with a higher priority and yield 
the cpu is needed. This is why if when lock_release() is called on a thread
it will immediately run the highest priority thread that was waiting for it.  

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

A race condition could arise in various situations. For instance, a race 
condition could occur when multiple threads are attempting to set priority 
at the same time or when a thread would be blocked by the scheduler while
changing its priority. The second could make a lower priority thread run
although the current thread was receving priority donation. To avoid a 
race conditions, we disabled interrupts for the duration of the function 
that updates the priorities.

Alternatively, we considered using a lock to prevent the scheduler from 
changing threads while a thread is in the critical section of the 
set_priority but I think disabling interrupts is a more clear solution.

We tried to test without disabling interrupts and all the test cases pass,
several times. So this case is similar to a heisenbug, so at least in theory 
I think it's a better solution.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

For strict priority scheduling we decided to change all the calls to push_back
to insert_orderd so we run the threads based on their priority. When adding
priority donation we changed our design as explanined on question B3.

For priority donation our original approach was to write the code in the 
sema_down and sema_up function as locks in pintos are implemented using 
semaphores. After further review of the design, we realized this was 
over-complicating the problem as we only needed priority donation for 
locks. Another possible approach is to put the logic for priority donation
in the thread struct, and handle things from there. This was a bit of overhead
and required a lot of changes in diffrent parts of the code. 

Finally, we decided to write all functionality in lock_acquire and lock_release.
This has the advantage that it is all in one place so it's easy to change
when we use the advanced scheduler. We handled nested donation using a list
of locks hold by each thread and we defined a lock's priority as the highest
priority of all threads waiting for the lock. This led to a simple design 
from a conceptual standpoint by having all the functionality in one place.  

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Struct Members:
Added to struct thread:
  /* Members for BSD Scheduler. */
  int nice;                           /* Nice value of the thread. */
  int32_t recent_cpu;                 /* Recent CPU value of the thread in 
                                         17.14 Fixed-Point representation. */

Static Variables in thread.c:

Load average for the system, stored in 17.14 Fixed-Point format.
int32_t load_avg;                   

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  58     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     B
12      8   4   0  61  60  59     A
16     12   4   0  60  60  59     B
20     12   8   0  60  59  59     A
24     16   8   0  59  59  59     C
28     16  12   0  59  59  58     B
32     16  12   4  59  58  58     A
36     20  12   4  58  58  58     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behaviour of your scheduler?

The scheduler specification did not say in which order the calculation of
priority for each thread and the increment of the recent cpu value should be 
done if both need doing on the same clock tick. I decided to increment the
recent cpu value of the current thread before calculating the priority of the 
threads so that the recent cpu value used in the priority calculation for the 
current thread is a more accurate value and it make more sense than using it 
in a calculation and then incrementing it.
This table matches the behaviour of my scheduler. Every tick the recent cpu 
value is incremented for the current thread and every 4 ticks the priorities
are re-calculated for each thread. The thread with the highest priority runs,
and when multiple threads have the same priority the one with the longest 
time since it was last scheduled is run.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Most of the BSD scheduler runs inside interrupt context. This is due to the 
specification requiring that the system load average and the recent cpu for 
each thread is calculated exactly every second. If these calculations were 
offloaded onto a kernel thread, the thread could be interrupted with a timer 
tick and then the calculations would not be done at the correct time.
The recalculation of thread priorities every 4 ticks also runs within the
interrupt context. This is because the recalculated priorities need to be 
finished before the scheduler runs so that correct thread runs next. If only 
some of the priorities were updated before the scheduler run, then the wrong 
thread could be chosen to run next.

Having all of the calculations done in the interrupt context is not ideal as
this prevents other interrupts occurring and means the thread that was 
interrupted has less time to run. However these calculations need to be done at
specific times and the values that they use should not change from when a value 
is calculated for one thread to another. The priorities calculations could be 
shifted to just before the thread yields on returning from the interrupt 
handler, but the calculations would still be done before the thread yields so
would not improve the performance from the way it is currently done.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the task, how might you choose to
>> refine or improve your design?

I feel that this design is good in terms of simplicity and readability. The
implementation is split well between a set of functions that can be clearly
understood as to what they do. All the functions are also grouped together in
one file. Macros provide and simple and efficient implementation of the fixed-
point operations that are required. 

A disadvantage would be that most of calculations required for the scheduling
is done in the interrupt context of the timer interrupt. This is not good so if
I were to have extra time, I would look to see if any of the calculations could
be done outside the interrupt context whilst still maintaining the correct
behaviour of the system.

>> C6: The assignment explains arithmetic for fixed-point mathematics in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point mathematics, that is, an abstract
>> data type and/or a set of functions or macros to manipulate
>> fixed-point numbers, why did you do so?  If not, why not?

To implement the fixed-point mathematics, I created a set of macros for all
of the arithmetic operations given in the specification. This was so that I 
could place the calculations within my code without having to continually
look them up. This also make the code more readable as the operations have 
names in the code rather than just looking at the implementation of each and 
having to find out what each one does. As the calculations are only written 
out once it meant that there was less risk of errors compared to typing each
once out multiple times, and if an error was made in the implementation of a 
calculation (which did occur), then it only needed to be changed in one place
and would be updated thought the code. 

The above could have been achieved by using a function for each calculation,
rather than a macro. I decided against this however as this would cause
overhead at runtime for the function call. By contrast the overhead of a macro 
is at compile time when the pre-processor runs and replaces the macros with the
defined values. 

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining tasks?

>> Any other comments?
